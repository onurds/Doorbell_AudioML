{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code uses Python with Keras (TensorFlow backend) for a 1D CNN model, along with scikit-learn for label encoding and data splitting, plus librosa for audio feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing audio files...\n",
      "Training the model...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785ms/step - accuracy: 0.5294 - loss: 5.6831 - val_accuracy: 0.6000 - val_loss: 11.1201\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6471 - loss: 10.6192 - val_accuracy: 0.6000 - val_loss: 8.4045\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6471 - loss: 9.5476 - val_accuracy: 0.6000 - val_loss: 1.3863\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5882 - loss: 5.9159 - val_accuracy: 0.4000 - val_loss: 2.3838\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.4706 - loss: 7.1869 - val_accuracy: 0.4000 - val_loss: 1.4891\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5882 - loss: 6.0578 - val_accuracy: 1.0000 - val_loss: 0.0243\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8235 - loss: 1.6381 - val_accuracy: 0.8000 - val_loss: 0.3055\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9412 - loss: 0.0932 - val_accuracy: 0.8000 - val_loss: 1.0870\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8235 - loss: 2.3585 - val_accuracy: 0.8000 - val_loss: 1.4999\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8235 - loss: 2.0939 - val_accuracy: 0.6000 - val_loss: 2.2164\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7647 - loss: 3.8241 - val_accuracy: 0.8000 - val_loss: 1.8615\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7647 - loss: 1.9818 - val_accuracy: 0.8000 - val_loss: 0.9920\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7647 - loss: 3.1620 - val_accuracy: 1.0000 - val_loss: 0.1383\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5882 - loss: 3.1003 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8824 - loss: 0.4934 - val_accuracy: 1.0000 - val_loss: 8.2556e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8235 - loss: 1.1636 - val_accuracy: 1.0000 - val_loss: 3.8594e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9412 - loss: 0.6549 - val_accuracy: 1.0000 - val_loss: 8.9639e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8824 - loss: 0.6015 - val_accuracy: 1.0000 - val_loss: 3.6607e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8235 - loss: 1.6015 - val_accuracy: 1.0000 - val_loss: 1.6594e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8824 - loss: 0.2405 - val_accuracy: 1.0000 - val_loss: 3.3855e-06\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9412 - loss: 0.2728 - val_accuracy: 1.0000 - val_loss: 5.5789e-06\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 1.0000 - val_loss: 1.7475e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9412 - loss: 0.3475 - val_accuracy: 1.0000 - val_loss: 2.9347e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8824 - loss: 0.3054 - val_accuracy: 1.0000 - val_loss: 1.0423e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 2.9780e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0143 - val_accuracy: 1.0000 - val_loss: 6.4828e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9412 - loss: 0.2655 - val_accuracy: 1.0000 - val_loss: 5.5215e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8824 - loss: 1.1748 - val_accuracy: 1.0000 - val_loss: 7.9711e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9412 - loss: 0.2713 - val_accuracy: 1.0000 - val_loss: 4.7206e-06\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9412 - loss: 0.2946 - val_accuracy: 1.0000 - val_loss: 9.5367e-08\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 1.4656e-06 - val_accuracy: 1.0000 - val_loss: 4.0531e-07\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9412 - loss: 0.4118 - val_accuracy: 1.0000 - val_loss: 2.0981e-06\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9412 - loss: 0.1377 - val_accuracy: 1.0000 - val_loss: 3.4571e-06\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 2.1037e-07 - val_accuracy: 1.0000 - val_loss: 5.4597e-06\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9412 - loss: 0.0973 - val_accuracy: 1.0000 - val_loss: 4.0769e-06\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9412 - loss: 0.8848 - val_accuracy: 1.0000 - val_loss: 7.8678e-07\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8824 - loss: 0.4857 - val_accuracy: 1.0000 - val_loss: 2.3842e-08\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 1.2523e-05 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 9.1160e-08 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9412 - loss: 0.9267 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 2.1037e-08 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0252 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 3.5230e-04 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9412 - loss: 0.4931 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 1.7094e-05 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 7.9940e-07 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Evaluating the model...\n",
      "Test accuracy: 100.00%\n",
      "Model saved as 'doorbell_classifier.keras'\n",
      "INFO:tensorflow:Assets written to: /var/folders/dy/k058gq41483gf6fpw0l7n7gc0000gn/T/tmpndfkuo86/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/dy/k058gq41483gf6fpw0l7n7gc0000gn/T/tmpndfkuo86/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/dy/k058gq41483gf6fpw0l7n7gc0000gn/T/tmpndfkuo86'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 40, 1), dtype=tf.float32, name='keras_tensor_55')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  13109578640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13109579984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13109583632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13109578448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13109580368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13109578256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13109576528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13109577680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1739309527.405110 2478864 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted and saved as 'doorbell_classifier.tflite'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1739309527.405286 2478864 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "I0000 00:00:1739309527.410309 2478864 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D\n",
    "\n",
    "def load_audio_data(data_path):\n",
    "   \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Target sampling rate\n",
    "    target_sr = 22050\n",
    "    \n",
    "    # For all subdirectories\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.wav', '.mp3')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Get the class label from the parent directory name\n",
    "                label = os.path.basename(root)\n",
    "                \n",
    "                try:\n",
    "                    # Load and resample the audio file\n",
    "                    audio, sample_rate = librosa.load(file_path, sr=target_sr)\n",
    "                    \n",
    "                    # Extract MFCC features\n",
    "                    mfccs = librosa.feature.mfcc(y=audio, sr=target_sr, n_mfcc=40)\n",
    "                    mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
    "                    \n",
    "                    # Append features and labels\n",
    "                    features.append(mfccs_scaled)\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    \n",
    "    # Building the 1D CNN model\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # First convolutional layer\n",
    "    model.add(Conv1D(64, 3, padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    model.add(Conv1D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_prediction(model, le, file_path):\n",
    "    \n",
    "    # Make prediction on a single audio file\n",
    "    \n",
    "    audio, sample_rate = librosa.load(file_path, sr=22050)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=40)\n",
    "    mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
    "    features = mfccs_scaled.reshape(1, mfccs_scaled.shape[0], 1)\n",
    "    predicted_vector = model.predict(features)\n",
    "    predicted_class_index = np.argmax(predicted_vector, axis=-1)\n",
    "    return le.inverse_transform(predicted_class_index)[0]\n",
    "\n",
    "def main():\n",
    "    \n",
    "    data_path = \"sounds\"\n",
    "    \n",
    "    print(\"Loading and preprocessing audio files...\")\n",
    "    features, labels = load_audio_data(data_path)\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    labels_encoded = le.fit_transform(labels)\n",
    "    labels_onehot = to_categorical(labels_encoded)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels_onehot, test_size=0.2, random_state=42, stratify=labels_onehot\n",
    "    )\n",
    "    \n",
    "    # Reshape the data for CNN\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    input_shape = (X_train.shape[1], 1)\n",
    "    model = build_model(input_shape, len(le.classes_))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    model.fit(X_train, y_train, \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs,\n",
    "              validation_data=(X_test, y_test),\n",
    "              verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating the model...\")\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "    \n",
    "    # Save the model\n",
    "    model.save('model/doorbell_classifier.h5')\n",
    "    print(\"Model saved as 'doorbell_classifier.h5'\")\n",
    "    \n",
    "    # Convert the model to TensorFlow Lite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save the TensorFlow Lite model\n",
    "    with open('model/doorbell_classifier.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(\"Model converted and saved as 'doorbell_classifier.tflite'\")\n",
    "    \n",
    "    return model, le\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, le = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting doorbell detection...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 178\u001b[0m\n\u001b[1;32m    175\u001b[0m     detector\u001b[38;5;241m.\u001b[39mstart_listening()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 175\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m detector \u001b[38;5;241m=\u001b[39m DoorbellDetector(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Start detection\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_listening\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 144\u001b[0m, in \u001b[0;36mDoorbellDetector.start_listening\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;66;03m# Get audio data from queue\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m         audio_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;66;03m# Add to buffer\u001b[39;00m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_buffer \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_buffer, audio_chunk\u001b[38;5;241m.\u001b[39mflatten())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "import queue\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pygame.mixer\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, messaging\n",
    "\n",
    "def initialize_firebase():\n",
    "    \n",
    "    if not firebase_admin._apps:\n",
    "        cred = credentials.Certificate('config/doorbell-notification-14f52-ddf5adc0791f.json')\n",
    "        return firebase_admin.initialize_app(cred)\n",
    "    return firebase_admin.get_app()\n",
    "\n",
    "\n",
    "class DoorbellDetector:\n",
    "    def __init__(self, model_path='model/doorbell_classifier.h5', threshold=0.7):\n",
    "        \n",
    "        # Load the trained model\n",
    "        self.model = load_model(model_path)\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Initialize label encoder\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(['background', 'doorbell'])\n",
    "        \n",
    "        # Initialize Firebase\n",
    "        self.firebase_app = initialize_firebase()\n",
    "        \n",
    "        # Audio parameters\n",
    "        self.sample_rate = 22050\n",
    "        self.chunk_duration = 0.5  # seconds\n",
    "        self.chunk_samples = int(self.sample_rate * self.chunk_duration)\n",
    "        self.channels = 1\n",
    "        \n",
    "        # Buffer for collecting audio chunks\n",
    "        self.audio_buffer = np.array([])\n",
    "        self.buffer_duration = 2.0\n",
    "        self.buffer_samples = int(self.sample_rate * self.buffer_duration)\n",
    "        \n",
    "        # Initialize audio queue\n",
    "        self.audio_queue = queue.Queue()\n",
    "        \n",
    "        # Initialize pygame for alert sound\n",
    "        pygame.mixer.init()\n",
    "        \n",
    "        # Instance variable to track the last notification time\n",
    "        self.last_notification_time = 0\n",
    "        self.notification_cooldown = 20  # Cooldown period to not send multiple notifications in (s)\n",
    "        \n",
    "    def process_audio(self, audio_data):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            if len(audio_data.shape) > 1:\n",
    "                audio_data = np.mean(audio_data, axis=1)\n",
    "            \n",
    "            # Ensure minimum length for FFT\n",
    "            if len(audio_data) < 2048:\n",
    "                audio_data = np.pad(audio_data, (0, 2048 - len(audio_data)))\n",
    "            \n",
    "            # Extract MFCC features\n",
    "            mfccs = librosa.feature.mfcc(\n",
    "                y=audio_data, \n",
    "                sr=self.sample_rate, \n",
    "                n_mfcc=40,\n",
    "                n_fft=1024,  \n",
    "                hop_length=512\n",
    "            )\n",
    "            mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
    "            return mfccs_scaled.reshape(1, mfccs_scaled.shape[0], 1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def audio_callback(self, indata, frames, time, status):\n",
    "        \n",
    "        if status:\n",
    "            print(f\"Status: {status}\")\n",
    "        self.audio_queue.put(indata.copy())\n",
    "\n",
    "    def predict_audio(self, audio_data):\n",
    "        \n",
    "        features = self.process_audio(audio_data)\n",
    "        if features is not None:\n",
    "            pred_probs = self.model.predict(features, verbose=0)[0]\n",
    "            pred_class = self.le.inverse_transform([np.argmax(pred_probs)])[0]\n",
    "            confidence = float(pred_probs[self.le.transform(['doorbell'])[0]])\n",
    "            return pred_class, confidence\n",
    "        return None, 0.0\n",
    "\n",
    "    def alert(self):\n",
    "        \n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Check if enough time has passed since the last notification\n",
    "        if current_time - self.last_notification_time < self.notification_cooldown:\n",
    "            print(\"Notification cooldown active, skipping alert...\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\nDOORBELL DETECTED!\")\n",
    "        \n",
    "        # Create message\n",
    "        message = messaging.Message(\n",
    "            notification=messaging.Notification(\n",
    "                title='Doorbell Alert',\n",
    "                body='Someone is at your door!'\n",
    "            ),\n",
    "            topic='doorbell_alerts'\n",
    "        )\n",
    "        \n",
    "        # Send message\n",
    "        try:\n",
    "            response = messaging.send(message)\n",
    "            print(f\"Successfully sent notification: {response}\")\n",
    "            # Update the last notification time\n",
    "            self.last_notification_time = current_time\n",
    "        except Exception as e:\n",
    "            print(f\"Error sending notification: {e}\")\n",
    "        \n",
    "        # Beep sound\n",
    "        # sd.play(np.sin(2 * np.pi * 440 * np.linspace(0, 0.1, 4410)), 44100)\n",
    "        # time.sleep(0.1)\n",
    "\n",
    "    def start_listening(self):\n",
    "        \n",
    "        try:\n",
    "            print(\"Starting doorbell detection...\")\n",
    "            \n",
    "            # Start audio stream\n",
    "            with sd.InputStream(\n",
    "                callback=self.audio_callback,\n",
    "                channels=self.channels,\n",
    "                samplerate=self.sample_rate,\n",
    "                blocksize=self.chunk_samples\n",
    "            ):\n",
    "                while True:\n",
    "                    try:\n",
    "                        # Get audio data from queue\n",
    "                        audio_chunk = self.audio_queue.get(timeout=1.0)\n",
    "                        \n",
    "                        # Add to buffer\n",
    "                        self.audio_buffer = np.append(self.audio_buffer, audio_chunk.flatten())\n",
    "                        \n",
    "                        # Keep buffer at desired length\n",
    "                        if len(self.audio_buffer) > self.buffer_samples:\n",
    "                            # Process when buffer is full\n",
    "                            pred_class, confidence = self.predict_audio(self.audio_buffer)\n",
    "                            \n",
    "                            if pred_class == 'doorbell' and confidence > self.threshold:\n",
    "                                self.alert()\n",
    "                            \n",
    "                            # Reset buffer with overlap\n",
    "                            overlap_samples = int(self.sample_rate * 0.1)  # 0.1 second overlap\n",
    "                            self.audio_buffer = self.audio_buffer[-overlap_samples:]\n",
    "                            \n",
    "                    except queue.Empty:\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in processing: {str(e)}\")\n",
    "                        continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in audio stream: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Create detector instance\n",
    "    detector = DoorbellDetector(threshold=0.7)\n",
    "    \n",
    "    # Start detection\n",
    "    detector.start_listening()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
